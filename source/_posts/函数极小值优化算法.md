---
title: 函数极小值优化算法
date: 2024-11-26 23:16:08
tags: Math
categories: Matrix-analyze
---

$g_i$代表当前步骤的梯度，$\alpha_i$代表当前的学习率,
$A_i$代表当前的$Hessian$矩阵（$\nabla^2F(x)|_{x=x_i}$）

# 共轭向量法



$$
\begin{aligned}
    &p_0=-\nabla F(x)|_{x=x_0}\\
    &g_0=\nabla F(x)|_{x=x_0}\\
    &\alpha_0=\frac{-g_0^Tp_0}{p_0^TA_0p_0}\\
    &x_1=x_0+\alpha_0p_0
\end{aligned}
$$

### while True: (其中1代表后一步的)
$$
\begin{aligned}
    &g_1=\nabla F(x)|_{x=x_1}\\
    &\beta_1=\frac{g_1^Tg_1}{g_0^Tg_0}\\
    &p_1=-g_1+\beta_1p_0\\
    &\alpha_1=\frac{-g_1^Tp_1}{p_1^TA_1p_1}\\
    &x_2=x_1+\alpha_1p_1
\end{aligned}
$$
# 最速下降法
$$
x_{k+1}=x_k-\alpha_kg_k
$$
### 最大稳定学习率（二次）
$$
\alpha<\frac{2}{\lambda_{\max}}
$$
$\lambda_{\max}$是$A$的最大特征值
# 沿直线最速下降算法

### while True: (其中1代表后一步的)
$$
\begin{aligned}
    &p_0=-\nabla F(x)|_{x=x_0}\\
    &g_0=\nabla F(x)|_{x=x_0}\\
    &\alpha_0=\frac{-g_0^Tp_0}{p_0^TA_0p_0}\\
    &x_1=x_0+\alpha_0p_0
\end{aligned}
$$

# 牛顿法

$$
x_{k+1}=x_k-A_k^{-1}g_k
$$